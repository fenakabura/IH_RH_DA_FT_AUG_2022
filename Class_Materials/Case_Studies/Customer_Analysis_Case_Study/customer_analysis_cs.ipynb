{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e88b73d",
   "metadata": {},
   "source": [
    "# Activites List\n",
    "<b>Important: for Activity 1, Activity 2 and  Activity 3 , please use the files [file1.csv](./Data/file1.csv), [file2.csv](./Data/file2.csv) and [file3.csv](./Data/file3.csv) from the [Data](./Data) folder.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50518b25",
   "metadata": {},
   "source": [
    "### Activity 1 (Monday)\n",
    "\n",
    "- Aggregate data into one Data Frame using Pandas. Pay attention that files may have different names for the same column. therefore, make sure that you unify the columns names before concating them. \n",
    "- Standardizing header names\n",
    "- Deleting and rearranging columns – delete the column customer as it is only a unique identifier for each row of data\n",
    "- Working with data types – Check the data types of all the columns and fix the incorrect ones (for ex. customer lifetime value and number of open complaints ). Hint: remove the percentage from the customer lifetime value and truncate it to an integer value.\n",
    "- clean the number of open complaints and extract the middle number which is changing between records. pay attention that the number of open complaints is a categorical feature.\n",
    "- Filtering data and Correcting typos – Filter the data in state and gender column to standardize the texts in those columns\n",
    "- Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dfcdc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. importing all the libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d319d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Creating a function to make all the column names lower case\n",
    "\n",
    "def lower_case_column_names(x):\n",
    "    x.columns=[i.lower() for i in x.columns]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e284293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Importing the files I'm using, assigning them to variables and cleaning the data\n",
    "\n",
    "file1 = pd.read_csv(\"Data/file1.csv\")\n",
    "file1 = lower_case_column_names(file1)\n",
    "file1.rename(columns={\"st\":\"state\", \"customer lifetime value\":\"customer_ltv\"}, inplace = True)\n",
    "# file1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a156238",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = pd.read_csv(\"Data/file2.csv\")\n",
    "file2 = lower_case_column_names(file2)\n",
    "file2.rename(columns={\"st\":\"state\", \"customer lifetime value\":\"customer_ltv\"}, inplace=True)\n",
    "file2 = file2[[\"customer\", \"state\", \"gender\", \"education\", \"customer_ltv\", \"income\", \"monthly premium auto\", \"number of open complaints\", \"policy type\", \"vehicle class\", \"total claim amount\"]]\n",
    "# file2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "61393497",
   "metadata": {},
   "outputs": [],
   "source": [
    "file3 = pd.read_csv(\"Data/file3.csv\")\n",
    "file3 = lower_case_column_names(file3)\n",
    "file3.rename(columns={\"customer lifetime value\":\"customer_ltv\"}, inplace=True)\n",
    "file3 = file3[[\"customer\", \"state\", \"gender\", \"education\", \"customer_ltv\", \"income\", \"monthly premium auto\", \"number of open complaints\", \"policy type\", \"vehicle class\", \"total claim amount\"]]\n",
    "# file3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ec90d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Creating a function to combine all the files together\n",
    "\n",
    "def combining_files():\n",
    "    return pd.concat([file1,file2,file3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db468a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combining_files()\n",
    "df.drop([\"customer\"], axis=1, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dc8051dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state ->  ['Washington' 'Arizona' 'Nevada' 'California' 'Oregon' 'Cali' 'AZ' 'WA'\n",
      " nan]\n",
      "gender ->  [nan 'F' 'M' 'Femal' 'Male' 'female']\n",
      "education ->  ['Master' 'Bachelor' 'High School or Below' 'College' 'Bachelors' 'Doctor'\n",
      " nan]\n",
      "policy type ->  ['Personal Auto' 'Corporate Auto' 'Special Auto' nan]\n",
      "vehicle class ->  ['Four-Door Car' 'Two-Door Car' 'SUV' 'Luxury SUV' 'Sports Car'\n",
      " 'Luxury Car' nan]\n"
     ]
    }
   ],
   "source": [
    "print(\"state -> \", df[\"state\"].unique())\n",
    "print(\"gender -> \", df[\"gender\"].unique())\n",
    "print(\"education -> \", df[\"education\"].unique())\n",
    "print(\"policy type -> \", df[\"policy type\"].unique())\n",
    "print(\"vehicle class -> \", df[\"vehicle class\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a48f5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'] = df['state'].replace(['AZ', 'WA', 'Cali'],['Arizona', 'Washington', 'California'])\n",
    "# df[\"state\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d063878b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'] = df['gender'].replace(['Femal', 'female'], 'F')\n",
    "df['gender'] = df['gender'].replace(['Male'], 'M')\n",
    "df['gender'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3c9b9",
   "metadata": {},
   "source": [
    "### Activity 2 (Tuesday)\n",
    "- Replacing null values – Replace missing values with means of the column (for numerical columns). Pay attention that the Income feature for instance has 0s which is equivalent to null values. (We assume here that there is no such income with 0 as it refers to missing values)\n",
    "Hint: numpy.nan is considered of float64 data type.\n",
    "- Bucketing the data - Write a function to replace column \"State\" to different zones. California as West Region, Oregon as North West, and Washington as East, and Arizona and Nevada as Central\n",
    "- (Optional) Standardizing the data – Use string functions to standardize the text data (lower case)\n",
    "\n",
    "<b>Important: for Activity 3 and Activity 4 , please use the [file Data_Marketing_Customer_Analysis_Round3.csv](./Data/Data_Marketing_Customer_Analysis_Round3.csv) from the [Data](./Data) folder.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45adea",
   "metadata": {},
   "source": [
    "\n",
    "### Activity 3 (Wednesday)\n",
    "\n",
    "- Get the numeric data into dataframe called `numerical` and categorical columns in a dataframe called `categoricals`.\n",
    "(You can use np.number and np.object to select the numerical data types and categorical data types respectively)\n",
    "- Now we will try to check the normality of the numerical variables visually\n",
    "  - Use seaborn library to construct distribution plots for the numerical variables\n",
    "  - Use Matplotlib to construct histograms\n",
    "  - Do the distributions for different numerical variables look like a normal distribution \n",
    "- For the numerical variables, check the multicollinearity between the features. Please note that we will use the column `total_claim_amount` later as the target variable.\n",
    "- Optional: Drop one of the two features that show a high correlation between them (greater than 0.9). If there is no pair of features that have a high correlation, then do not drop any features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe72353",
   "metadata": {},
   "source": [
    "### Activity 4 (Thursday)\n",
    "\n",
    "- Show a plot of the total number of responses.\n",
    "- Show a plot of the response by the sales channel.\n",
    "- Show a plot of the response by the total claim amount.\n",
    "- Show a plot of the response by income.\n",
    "- (Optional) Don't limit your creativity!  plot any interesting findings/insights that describe some interesting facts about your data set and its variables.\n",
    "- Plot the Correlation Heatmap.\n",
    "- Clean your notebook and make it a readible and presentable with a good documentation that summarizes the Data Cleaning, Exploration(including plots) Steps that you have performed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
